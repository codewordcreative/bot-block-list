# Bot block list - what it says on the tin
I've been collating various bots worth blocking from various sources (listed below) and compiling them in .htaccess rules. The entries/rules in the list can be applied to non-Apache servers - it's just regex so human-readable. I've made extensive comments to help people decide what to implement and how.

## Collaboration welcomed!
If anyone would like to contribute - be it corrections, additions, or any other assistance - please do. Every time I update my list, there are always some big baddies I need to add in.

## Key principles

### Why block? What to block?
All crawled content has an immediate impact: data transfer.

#### Crawlers I'd never add
  - Legitimate search engines (Google, Bing, Yandex, DuckDuckGo, etc...)
  - Any crawler that is unlikely to act without user initiation
  - Security-related tools ensuring websites are safe to visit
  - Any platform looking for Open Graph or similar information when a link is shared on their platform
  - Other legitimate purposes, e.g. tools ensuring proper website function

#### Data collection and processing  
Automated data collection at scale is sometimes helpful, but often not.
  - Where data is stored and referenced, such as that has an additional impact each time it is referenced.
  - Where data is stored, referenced, and used by a third party (most often for marketing purposes), this has an unknown additional impact.
    * In the case of marketing and email scrapers, it may lead to spam emails.
    * Personal data may be stored, processed, exposed to and ultimately used by nefarious third parties.
   
#### Unused SEO tools
Many competing SEO companies crawl the same content, hunting for backlinks, looking for content placement opportunities, or benchmarking the competition. This is inherently wasteful if the data does not benefit *you*.
  - Where you are not using this tool, it's pointless.
  - Where your competitors are using this tool, it is more likely to disadvantage you.
  - It will always use bandwidth.
  
#### Training large language models (LLMs) and content generation (generative AI)
Data storage and processing for LLMs and its later referencing to create content has impacts on content originators, society, and the environment.
  - Data collected is stored and/or processed to train models, meaning that beyond the storage or training process, it enlarges the model itself - ordinarily increasing the impact each time the model is used.
  - Copyright is directly violated in the act of training, regardless if visible to the human eye or in data analysis of anything later generated by that model.
  - Theft and imitation devalues creativity, makes it harder for creatives to be duly recognised for their originality and talent, and threatens creativity and culture itself.
  - Theft and imitation devalues academia, scientists, doctors, programmers, translators, and every other skilled profession out there in a similar way, with analogous impacts.
  - Promotion of AI encourages overuse and dependence by design, slowing the development of real skills of currently free systems that will eventually be exclusively pay-to-play.
  - Every use has scaling negative environmental impact. A lack of industry transparency means we cannot be sure what that is. But we've seen the data centres and power plants spring up.

## How to block?
Assuming you are on an Apache server (e.g., WordPress and many other setups will usually be using Apache), adapt my rules as per your preferences (anything starting with a # is a comment and can be deleted!) and put them in your .htaccess file, near the top.

## What about Robots.txt?
A small number of AI agencies will observe robots.txt, but many don't. Robots.txt is still necessary as it is the only way to block Google's AI training without touching your ability to rank in Google Search or AI summaries (yep, appearing in Google's AI summaries doesn't require you to let yourself get scraped to train Gemini).

## Differences to Cloudflare
### The block list
Cloudflare's list does not go anything like as far. They have also marked some bots as verified, therefore not blocked, despite them being known to crawl for AI training purposes

